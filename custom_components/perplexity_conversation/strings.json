{
  "config": {
    "step": {
      "user": {
        "data": {
          "api_key": "[%key:common::config_flow::data::api_key%]"
        }
      }
    },
    "error": {
      "cannot_connect": "[%key:common::config_flow::error::cannot_connect%]",
      "invalid_auth": "[%key:common::config_flow::error::invalid_auth%]",
      "unknown": "[%key:common::config_flow::error::unknown%]"
    }
  },
  "options": {
    "step": {
      "init": {
        "data": {
          "prompt": "Instructions",
          "chat_model": "[%key:common::generic::model%]",
          "max_tokens": "Maximum tokens to return in response",
          "temperature": "Temperature",
          "top_p": "Top P",
          "llm_hass_api": "[%key:common::config_flow::data::llm_hass_api%]",
          "recommended": "Recommended model settings",
          "reasoning_effort": "Reasoning effort"
        },
        "data_description": {
          "prompt": "Instruct how the LLM should respond. This can be a template.",
          "reasoning_effort": "How many reasoning tokens the model should generate before creating a response to the prompt (for certain reasoning models)"
        }
      }
    },
    "error": {
      "model_not_supported": "This model is not supported, please select a different model"
    }
  },
  "selector": {
    "reasoning_effort": {
      "options": {
        "low": "Low",
        "medium": "Medium",
        "high": "High"
      }
    }
  },
  "services": {},
  "exceptions": {
    "invalid_config_entry": {
      "message": "Invalid config entry provided. Got {config_entry}"
    }
  }
}
